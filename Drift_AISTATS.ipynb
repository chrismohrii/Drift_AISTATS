{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Basic Python imports"
      ],
      "metadata": {
        "id": "5Iz3CK87RQFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oQo1ju15NCti",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1677190127664,
          "user_tz": 480,
          "elapsed": 59,
          "user": {
            "displayName": "Pranjal Awasthi",
            "userId": "08866363694134506097"
          }
        }
      },
      "outputs": [],
      "source": [
        "\"Basic Python Imports\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import bernoulli\n",
        "from scipy.stats import beta\n",
        "import random\n",
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import warnings\n",
        "from six.moves import xrange\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy.sparse.linalg import eigs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrepancy computation code"
      ],
      "metadata": {
        "id": "Hi1sVYcgRLTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Discrepancy computation\"\n",
        "\n",
        "\n",
        "def compute_disc(xs, ys, xt, yt, outer_iters = 100, inner_iters=1000, tol=1e-5, lr = 0.1):\n",
        "\n",
        "  \"\"\" Given features and labels from source (xs, ys) and target (xt, yt), the function outputs the computed discrepancy between the two domains. \n",
        "  The DC programming method is used to approximate the discrepancy.\n",
        "\n",
        "  Arguments: \n",
        "    xs: source feature data of size m x d\n",
        "    ys: source label data of size m x 1\n",
        "    xt: target feature data of size n x d\n",
        "    yt: target label data of size n x 1\n",
        "    outer_iters: # outer iterations of DC programming.\n",
        "    inner_iters: # inner iterations of gradient descent for each step of DC programming.\n",
        "    tol: the objective function improvement tolerance value for the inner loop.\n",
        "    lr: learning rate for the inner loop\n",
        "  \"\"\" \n",
        "\n",
        "  m = xs.shape[0]\n",
        "  n = xt.shape[0]\n",
        "  d = xs.shape[1]\n",
        "\n",
        "  w = np.random.normal(size=(d,1))\n",
        "  w /= np.linalg.norm(w)\n",
        "\n",
        "  outer_obj_val = np.inf\n",
        "\n",
        "  loss = []\n",
        "\n",
        "  for i in range(outer_iters):\n",
        "\n",
        "    w0 = w\n",
        "    w = np.random.normal(size=(d,1))\n",
        "    w /= np.linalg.norm(w)\n",
        "    ypred1 = np.matmul(xs, w0)\n",
        "    ypred2 = np.matmul(xt, w0)\n",
        "\n",
        "    curr_obj_val = np.linalg.norm(ypred1 - ys)**2/m - np.linalg.norm(ypred2 - yt)**2/n\n",
        "    loss.append(curr_obj_val)\n",
        "    if abs(curr_obj_val - outer_obj_val) <= tol:\n",
        "      return curr_obj_val\n",
        "    outer_obj_val = curr_obj_val\n",
        "\n",
        "\n",
        "    \"\"\" optimize for w \"\"\"\n",
        "    inner_obj_val = np.inf\n",
        "    for j in range(inner_iters):\n",
        "      ypred = (np.matmul(xs, w) - ys).squeeze()\n",
        "      M = np.matmul(np.diag(ypred), xs)\n",
        "      grad = np.sum(M, axis=0)/m\n",
        "      grad = np.expand_dims(grad, axis=1)\n",
        "\n",
        "      ypred2 = (np.matmul(xt, w0) - yt).squeeze()\n",
        "      M = np.matmul(np.diag(ypred2), xt)\n",
        "      grad2 = np.sum(M, axis=0)/n\n",
        "      grad2 = np.expand_dims(grad2, axis=1)\n",
        "\n",
        "      grad -= grad2\n",
        "\n",
        "      w -= lr*grad\n",
        "      if np.linalg.norm(w) > 1:\n",
        "        w /= np.linalg.norm(w)\n",
        "\n",
        "      ypred1 = np.matmul(xs, w)\n",
        "      ypred2 = np.matmul(xt, w0)\n",
        "      ypred3 = np.matmul(xt, w)\n",
        "\n",
        "      curr_obj_val = 0.5*np.linalg.norm(ypred1 - ys)**2/m - np.sum(np.multiply(ypred2, ypred3))/n\n",
        "      if abs(curr_obj_val - inner_obj_val) <= tol:\n",
        "        break\n",
        "      inner_obj_val = curr_obj_val\n",
        "\n",
        "\n",
        "  return -outer_obj_val\n",
        "\n",
        "\n",
        "def get_dbars(xs, ys, xt, yt, ms):\n",
        "  \"\"\" Given data from multiple sources and a target domain, returns a list of discrepancy values between each source and the target.\n",
        "\n",
        "  Arguments:\n",
        "    xs: source data of size m x d, where m is the total number of points from all the sources\n",
        "    ys: source data of size m x 1, where m is the total number of points from all the sources\n",
        "    xt: target feature data of size n x d\n",
        "    yt: target label data of size n x 1\n",
        "    ms: a list containing the source data and target data sizes\n",
        "  \"\"\"\n",
        "  dbars_drift = []\n",
        "  for t in range(len(ms) - 1):\n",
        "    n_t = int(np.sum(ms[:t])) # From defintion of n_t\n",
        "    t_disc = compute_disc(xs[n_t : n_t + ms[t]], ys[n_t : n_t + ms[t]], xt, yt)\n",
        "    dbars_drift.append(abs(t_disc))\n",
        "  return dbars_drift\n"
      ],
      "metadata": {
        "id": "B6zDuLQCNb-3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1677190130725,
          "user_tz": 480,
          "elapsed": 113,
          "user": {
            "displayName": "Pranjal Awasthi",
            "userId": "08866363694134506097"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main alternate minimization procedure"
      ],
      "metadata": {
        "id": "shsllpqeRYiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def altmin(xs, xt, ys, yt, x_trgt_test, y_trgt_test, x_trgt_val, y_trgt_val, lambda_1, lambda_2, lambda_3, dbar, p0, lr, ms, maxiters=100, niters=1000, q_init=None, tol=1e-3):\n",
        "  \"\"\" Given data from multiple sources and a target domain, the procedure runs the alternate minimization algorithm to find the best fit model for the \n",
        "  target data and reports the resulting test error.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    xs: source feature data of size m x d\n",
        "    ys: source label data of size m x 1\n",
        "    xt: target feature data of size n x d\n",
        "    yt: target label data of size n x 1\n",
        "    x_trgt_test: target feature test data\n",
        "    y_trgt_test: target label test data\n",
        "    x_trgt_val: target feature validation data\n",
        "    y_trgt_val: target label validation data\n",
        "    lambda_1: regularizer term for the infinity norm of q\n",
        "    lambda_2: regularizer term for the distance from starting point (p0)\n",
        "    lambda_3: regularizer term for the squared l2 norm of q\n",
        "    dbar: a list of discrepancy values between each source and target\n",
        "    p0: the starting distribution over the data points\n",
        "    lr: the learning rate for gradient descent\n",
        "    ms: a list containing the source data and target data sizes\n",
        "    maxiters: the maximum number of iterations of alternate minimization\n",
        "    niters: the maximum number of iterations of the q optimization step\n",
        "    q_init: the initial value of q\n",
        "    tol: the objective function tolerance value for stopping criteria.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  print(\"Calling altmin\")\n",
        "  \"\"\" initialize q \"\"\"\n",
        "  m = xs.shape[0]\n",
        "  n = xt.shape[0]\n",
        "  x = np.vstack((xs, xt))\n",
        "  y = np.vstack((ys, yt))\n",
        "\n",
        "  if q_init is None:\n",
        "    q = np.random.uniform(size=m+n)\n",
        "    q = -np.log(q)\n",
        "    q /= np.sum(q)\n",
        "  else:\n",
        "    q = q_init\n",
        "\n",
        "  loss = []\n",
        "  prev_obj = np.inf\n",
        "  T = 0\n",
        "  best_h = []\n",
        "\n",
        "\n",
        "  for i in range(maxiters):\n",
        "\n",
        "    indices = []\n",
        "    for j in range(m+n):\n",
        "      if q[j] > 0:\n",
        "        indices.append(j)\n",
        "    clf = Ridge(alpha=lambda_1 * np.max(q), solver='cholesky')\n",
        "    clf.fit(x[indices,:], y[indices], sample_weight=q[indices])\n",
        "\n",
        "    curr_h = clf.coef_\n",
        "    ypred = clf.predict(x)\n",
        "\n",
        "    \"\"\" get current objective value \"\"\"\n",
        "    curr_obj = compute_obj_val(xs, xt, ys, yt, ypred, q, lambda_1, lambda_2, lambda_3, p0, dbar, np.linalg.norm(curr_h)**2, ms)\n",
        "    print(curr_obj)\n",
        "    if abs(curr_obj - prev_obj) <= tol:\n",
        "      best_h.append(clf)\n",
        "      T += 1\n",
        "    else:\n",
        "      T = 0\n",
        "\n",
        "    if i == maxiters - 1:\n",
        "      best_h.append(clf)\n",
        "\n",
        "    prev_obj = curr_obj\n",
        "    if T == 5:\n",
        "      break\n",
        "\n",
        "\n",
        "    loss.append(curr_obj)\n",
        "\n",
        "\n",
        "    \"\"\" Update q via gradient descent \"\"\"\n",
        "    init_lr = lr\n",
        "    best_inner_obj = curr_obj\n",
        "    best_q = q\n",
        "    iloss = ((y-ypred)**2).squeeze()\n",
        "    for j in range(niters):\n",
        "      grad = get_gradient(xs, xt, q, iloss, lambda_1, lambda_2, lambda_3, p0, np.linalg.norm(curr_h)**2, dbar, ms)\n",
        "      grad /= np.linalg.norm(grad)\n",
        "      q = q - init_lr*grad/(j+1)\n",
        "      q = project_simplex(q)\n",
        "      obj = compute_obj_val(xs, xt, ys, yt, ypred, q, lambda_1, lambda_2, lambda_3, p0, dbar, np.linalg.norm(curr_h)**2, ms)\n",
        "      if obj < best_inner_obj:\n",
        "        best_inner_obj = obj\n",
        "        best_q = q\n",
        "\n",
        "    q = best_q\n",
        "  indices = []\n",
        "  for j in range(m+n):\n",
        "    if q[j] > 0:\n",
        "      indices.append(j)\n",
        "  clf = Ridge(solver='cholesky')\n",
        "  clf.fit(x[indices,:], y[indices], sample_weight=q[indices])\n",
        "\n",
        "  best_h.append(clf)\n",
        "\n",
        "  \"\"\" use val data for model selection \"\"\"\n",
        "  best_error = np.inf\n",
        "  best_model = []\n",
        "  for clf in best_h:\n",
        "    ypred = clf.predict(x_trgt_val)\n",
        "    error = np.linalg.norm(y_trgt_val - ypred)/len(ypred)\n",
        "    if error < best_error:\n",
        "      best_error = error\n",
        "      best_model = clf\n",
        "\n",
        "  ypred = best_model.predict(x_trgt_test)\n",
        "  test_error = np.linalg.norm(y_trgt_test - ypred)**2/len(ypred)\n",
        "  print(\"test error = {}\".format(test_error))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return test_error, q\n",
        "\n",
        "\n",
        "def compute_obj_val(xs, xt, ys, yt, ypred, q, lambda_1, lambda_2, lambda_3, p0, dbars, hnorm, ms):\n",
        "  \"\"\" Helper function. Computes the objective value of a given solution.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    xs: source feature data of size m x d\n",
        "    ys: source label data of size m x 1\n",
        "    xt: target feature data of size n x d\n",
        "    yt: target label data of size n x 1\n",
        "    ypred: model predictions on all the data\n",
        "    q: current solution\n",
        "    lambda_1: regularizer term for the infinity norm of q\n",
        "    lambda_2: regularizer term for the distance from starting point (p0)\n",
        "    lambda_3: regularizer term for the squared l2 norm of q\n",
        "    p0: the starting distribution over the data points\n",
        "    dbars: a list of discrepancy values between each source and target\n",
        "    hnorm: the squared l2 norm of the Ridge regression predictor\n",
        "    ms: a list containing the source data and target data sizes\n",
        "  \"\"\"\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  y = np.vstack((ys, yt))\n",
        "  for i in range(len(q)):\n",
        "    loss += (y[i]-ypred[i])**2 * q[i]\n",
        "\n",
        "  loss += lambda_3 * np.linalg.norm(q)**2\n",
        "\n",
        "  loss += lambda_2 * np.linalg.norm(q - p0, ord=1)\n",
        "\n",
        "  loss += lambda_1 * hnorm * np.max(q)\n",
        "\n",
        "  for t in range(len(ms) - 1):\n",
        "    # Compute q_t bar\n",
        "    n_t = int(np.sum(ms[:t])) # From defintion of n_t\n",
        "    q_t_bar = np.sum(q[n_t : n_t + ms[t]])\n",
        "    loss += dbars[t] * q_t_bar\n",
        "\n",
        "  return loss  \n",
        "\n",
        "\n",
        "def get_gradient(xs, xt, q, loss, lambda_1, lambda_2, lambda_3, p0, hnorm, dbars, ms):\n",
        "  \"\"\" Helper function. Computes the gradient of the objective value of a given solution with respoect to the q variable.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    xs: source feature data of size m x d\n",
        "    xt: target feature data of size n x d\n",
        "    q: current solution\n",
        "    loss: model loss values on all the data\n",
        "    lambda_1: regularizer term for the infinity norm of q\n",
        "    lambda_2: regularizer term for the distance from starting point (p0)\n",
        "    lambda_3: regularizer term for the squared l2 norm of q\n",
        "    p0: the starting distribution over the data points\n",
        "    hnorm: the squared l2 norm of the Ridge regression predictor\n",
        "    dbars: a list of discrepancy values between each source and target\n",
        "    ms: a list containing the source data and target data sizes\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  m = xs.shape[0]\n",
        "  n = xt.shape[0]\n",
        "\n",
        "  \"\"\" get loss part of gradient \"\"\"\n",
        "  loss_grad = loss\n",
        "\n",
        "  \"\"\" get dbar part of gradient \"\"\"\n",
        "  dbar_grad = np.zeros((np.sum(ms)))\n",
        "  for t in range(len(ms) - 1):\n",
        "    n_t = int(np.sum(ms[:t])) # From defintion of n_t\n",
        "    dbar_grad[n_t : n_t + ms[t]] = dbars[t]*np.ones(ms[t])\n",
        "\n",
        "  \"\"\" get lambda_1 part of gradient \"\"\"\n",
        "  lambda_1_grad = np.zeros((m+n))\n",
        "  i1 = np.argmax(q)\n",
        "  lambda_1_grad[i1] = lambda_1*hnorm\n",
        "\n",
        "  \"\"\" get lambda_2 part of gradient \"\"\"\n",
        "  lambda_2_grad = np.zeros((m+n))\n",
        "  lambda_2_grad = lambda_2*np.sign(q - p0)\n",
        "\n",
        "  \"\"\" get lambda_3 part of gradient \"\"\"\n",
        "  lambda_3_grad = np.zeros((m+n))\n",
        "  lambda_3_grad = 2*lambda_3 * q\n",
        "\n",
        "\n",
        "\n",
        "  return np.asarray(loss_grad + dbar_grad + lambda_1_grad + lambda_2_grad + lambda_3_grad, np.float32)\n",
        "\n",
        "\n",
        "def project_simplex(q):\n",
        "\n",
        "  \"\"\" Projects a given vector onto the unit simplex.\n",
        "  \"\"\"\n",
        "\n",
        "  d = q.shape[0]\n",
        "\n",
        "  qprime = -np.sort(-q)\n",
        "\n",
        "  K = 1\n",
        "  Sum = []\n",
        "  Sum.append(qprime[K-1])\n",
        "  for i in range(2, d+1):\n",
        "    Sum.append(Sum[-1] + qprime[i-1])\n",
        "    if (Sum[-1]-1)/i < qprime[i-1]:\n",
        "      K = i\n",
        "\n",
        "  tau = (Sum[K-1]-1)/K\n",
        "  q = q-tau\n",
        "  for i in range(d):\n",
        "    q[i] = max(q[i],0)\n",
        "\n",
        "  q /= np.sum(q)\n",
        "\n",
        "\n",
        "  return q"
      ],
      "metadata": {
        "id": "w0mGaKRKRJzj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1677190125244,
          "user_tz": 480,
          "elapsed": 464,
          "user": {
            "displayName": "Pranjal Awasthi",
            "userId": "08866363694134506097"
          }
        }
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Usage"
      ],
      "metadata": {
        "id": "ym3xYI0FUoUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_simulated_data(d, ms, eps1, eps2, alpha=0.5, sigma=0.1, random_w=False, normalize_w_t=True):\n",
        "  \"\"\"\n",
        "  Generates simulated data for D_1 to D_{T+1} Gaussian distributions.\n",
        "\n",
        "  Parameters:\n",
        "  d (int): Dimension of the data.\n",
        "  ms (int list): Sample size for each of the T+1 distributions.\n",
        "  eps1 (float list): Standard deviation for each of the T+1 distributions.\n",
        "  eps2 (float list): Measures difference between w for D_1 to D_T (has size T).\n",
        "  alpha (float): Fraction of each D_1 to D_T to leave without noise.\n",
        "  sigma (float): Standard deviation of noise to add to labels. \n",
        "\n",
        "  Returns:\n",
        "  x_d1T_train: Combined X for each of the D_1 to D_T distributions\n",
        "  y_d1T_train: Combined Y for each of the D_1 to D_T distributions\n",
        "  x_trgt_train: X train for the D_{T+1} distribution\n",
        "  y_trgt_train Y train for the D_{T+1} distribution\n",
        "  x_trgt_test: X test for the D_{T+1} distribution\n",
        "  y_trgt_test: Y test for the D_{T+1} distribution\n",
        "  x_trgt_val: X validation for the D_{T+1} distribution\n",
        "  y_trgt_val: Y validation for the D_{T+1} distribution\n",
        "  \"\"\"\n",
        "  T = len(ms) - 1 \n",
        "  m1T_sum = np.sum(ms[:T])\n",
        "\n",
        "  # Will hold X for each of D_1 to D_T\n",
        "  x_d1T_train = []\n",
        "\n",
        "  for i in range(T):\n",
        "    x_d1T_train.append(np.array(np.random.normal(scale=eps1[i], size=(ms[i],d)), np.float32))\n",
        "\n",
        "  x_trgt_train = np.array(np.random.normal(scale=eps1[T], size=(ms[T],d)), np.float32)\n",
        "\n",
        "  x_trgt_test = np.array(np.random.normal(scale=eps1[T], size=(int(10*m1T_sum),d)), np.float32)\n",
        "  x_trgt_val = np.array(np.random.normal(scale=[T], size=(100,d)), np.float32)\n",
        "\n",
        "  ws = []\n",
        "  w_trgt = np.random.normal(size=(d,1))\n",
        "  w_trgt /= np.linalg.norm(w_trgt)\n",
        "  w = np.random.normal(size=(d,1))\n",
        "  w /= np.linalg.norm(w)\n",
        "  ws.append(w_trgt)\n",
        "  for i in range(T):\n",
        "    if random_w:\n",
        "      w = np.random.normal(size=(d,1))\n",
        "    w_i = w_trgt + eps2[i]*w\n",
        "    if normalize_w_t:\n",
        "      w_i /= np.linalg.norm(w_i)\n",
        "    ws.insert(-1, w_i)\n",
        "\n",
        "\n",
        "  y_d1T_train = []\n",
        "  for i in range(T):\n",
        "    y_d1T_train.append(np.matmul(x_d1T_train[i], ws[i]) + np.random.normal(scale=sigma, size=(ms[i], 1)))\n",
        "  y_trgt_train = np.matmul(x_trgt_train, ws[T]) + np.random.normal(scale=sigma, size=(ms[T], 1))\n",
        "  y_trgt_test = np.matmul(x_trgt_test, ws[T]) + np.random.normal(scale=sigma, size=(10*m1T_sum, 1))\n",
        "  y_trgt_val = np.matmul(x_trgt_val, ws[T]) + np.random.normal(scale=sigma, size=(100, 1))\n",
        "\n",
        "  # introduce noise in D_1 to D_T\n",
        "  for i in range(T):\n",
        "    for j in range(int(alpha*ms[i]),ms[i]):\n",
        "      y_d1T_train[i][j] = np.linalg.norm(ws[i])**2       \n",
        "      x_d1T_train[i][j, :] = -100*ws[i].squeeze()\n",
        "\n",
        "  # Flatten first T samples\n",
        "  x_d1T_train = np.array([x for s_t in x_d1T_train for x in s_t])\n",
        "  y_d1T_train = np.array([y for s_t in y_d1T_train for y in s_t])\n",
        "\n",
        "  return x_d1T_train, y_d1T_train, x_trgt_train, y_trgt_train, x_trgt_test, y_trgt_test, x_trgt_val, y_trgt_val\n",
        "\n",
        "\n",
        "\n",
        "# define data generation parameters\n",
        "num_dims = 10\n",
        "ms = [100]*2\n",
        "standard_devs = [.1, 10] # eps1\n",
        "w_dists_from_trgt = [1] # eps2\n",
        "\n",
        "# generate data\n",
        "\n",
        "xs, ys, xt, yt, x_trgt_test, y_trgt_test, x_trgt_val, y_trgt_val = generate_simulated_data(num_dims, ms, standard_devs, w_dists_from_trgt, alpha=1)\n",
        "\n",
        "# compute discrepancies\n",
        "\n",
        "dbars = get_dbars(xs, ys, xt, yt, ms)\n",
        "\n",
        "\n",
        "# set up altmin parameters\n",
        "\n",
        "lambda_1 = 100\n",
        "lambda_2 = 0.1\n",
        "lambda_3 = 1000\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "m = xs.shape[0]\n",
        "n = xt.shape[0]\n",
        "p0 = np.zeros(m+n)\n",
        "p0[m:m+n] = 1.0\n",
        "p0 /= np.sum(p0)\n",
        "\n",
        "\n",
        "# call alternate minimization procedure\n",
        "test_error, q = altmin(xs, xt, ys, yt, x_trgt_test, y_trgt_test, x_trgt_val, y_trgt_val, lambda_1, lambda_2, lambda_3, dbars, p0, lr, ms, maxiters=100, niters=1000, q_init=None, tol=1e-3)\n"
      ],
      "metadata": {
        "id": "rMhcxCvKUnmy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1677191072141,
          "user_tz": 480,
          "elapsed": 129775,
          "user": {
            "displayName": "Pranjal Awasthi",
            "userId": "08866363694134506097"
          }
        },
        "outputId": "463176dc-dde3-4c14-f5f1-b5ff91f0f7bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling altmin\n",
            "[201.85463555]\n",
            "[78.6488714]\n",
            "[32.64781729]\n",
            "[19.80845977]\n",
            "[15.30804799]\n",
            "[13.66979927]\n",
            "[13.45035305]\n",
            "[13.25518565]\n",
            "[13.07684972]\n",
            "[12.91055175]\n",
            "[12.75548194]\n",
            "[12.61088836]\n",
            "[12.47605778]\n",
            "[12.35032998]\n",
            "[12.23617096]\n",
            "[12.13046561]\n",
            "[12.03190018]\n",
            "[11.94000018]\n",
            "[11.85706134]\n",
            "[11.78219484]\n",
            "[11.71229916]\n",
            "[11.6487916]\n",
            "[11.59145666]\n",
            "[11.5397616]\n",
            "[11.49364667]\n",
            "[11.45222668]\n",
            "[11.41437175]\n",
            "[11.37893591]\n",
            "[11.34618975]\n",
            "[11.31636598]\n",
            "[11.28881958]\n",
            "[11.26309509]\n",
            "[11.23910999]\n",
            "[11.21675669]\n",
            "[11.19591828]\n",
            "[11.17656088]\n",
            "[11.15900604]\n",
            "[11.14292606]\n",
            "[11.12837265]\n",
            "[11.11559337]\n",
            "[11.10444318]\n",
            "[11.09422969]\n",
            "[11.08490758]\n",
            "[11.07626063]\n",
            "[11.06856207]\n",
            "[11.06139285]\n",
            "[11.05470704]\n",
            "[11.04870922]\n",
            "[11.0432428]\n",
            "[11.03829898]\n",
            "[11.0338459]\n",
            "[11.02975421]\n",
            "[11.02591718]\n",
            "[11.02239359]\n",
            "[11.01917814]\n",
            "[11.01629949]\n",
            "[11.01372727]\n",
            "[11.01136915]\n",
            "[11.00928026]\n",
            "[11.00763046]\n",
            "[11.00617421]\n",
            "[11.00494795]\n",
            "[11.00389801]\n",
            "[11.00292169]\n",
            "[11.00207172]\n",
            "[11.00134098]\n",
            "[11.00093196]\n",
            "[11.00084562]\n",
            "test error = 0.022428921355628146\n"
          ]
        }
      ]
    }
  ]
}